{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ab668-8455-42c8-aa8f-ea6ceef5c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ğŸš€ 1ï¸âƒ£ Volume de Dados Ainda Pequeno\n",
    "Mesmo que 500.000 linhas pareÃ§am muitas, nÃ£o Ã© suficiente para demonstrar a vantagem do PySpark.\n",
    "\n",
    "âœ… Pandas Ã© eficiente para datasets que cabem em RAM.\n",
    "âœ… PySpark sÃ³ mostra vantagem quando os dados sÃ£o demasiado grandes para a memÃ³ria do sistema.\n",
    "\n",
    "ğŸ”´ SoluÃ§Ã£o: Experimenta com 50 milhÃµes de registos ou mais!\n",
    "Basta mudares:\n",
    "\n",
    "python\n",
    "Copiar\n",
    "Editar\n",
    "dados_alunos_grande = pd.DataFrame({\n",
    "    \"id_aluno\": list(range(1, 50000001)),  # 50 milhÃµes de alunos\n",
    "ğŸš€ 2ï¸âƒ£ Overhead do PySpark\n",
    "PySpark tem um custo inicial para configurar sessÃµes, distribuir tarefas e otimizar consultas.\n",
    "\n",
    "âœ… Para pequenos datasets, este overhead pode tornar PySpark mais lento.\n",
    "âœ… Para grandes volumes de dados, este overhead compensa porque distribui o processamento.\n",
    "\n",
    "ğŸ”´ SoluÃ§Ã£o: Usa um cluster Spark real (nÃ£o local) para ver o verdadeiro poder do PySpark.\n",
    "\n",
    "ğŸš€ 3ï¸âƒ£ Uso de Apenas 1 MÃ¡quina (Spark Local)\n",
    "Se o Spark estÃ¡ a correr localmente, nÃ£o estÃ¡ a distribuir o processamento.\n",
    "\n",
    "âœ… Pandas usa toda a RAM e CPU da mÃ¡quina.\n",
    "âœ… PySpark, em modo local, nÃ£o escala automaticamente.\n",
    "\n",
    "ğŸ”´ SoluÃ§Ã£o: Se tiveres um cluster com vÃ¡rias mÃ¡quinas, PySpark escala melhor.\n",
    "\n",
    "ğŸš€ 4ï¸âƒ£ Pandas Usa OtimizaÃ§Ãµes Internas\n",
    "Pandas Ã© altamente otimizado para operaÃ§Ãµes em memÃ³ria, enquanto PySpark usa RDDs e DataFrames distribuÃ­dos, que tÃªm alguma latÃªncia extra.\n",
    "\n",
    "âœ… Se os dados cabem na RAM, Pandas pode ser mais rÃ¡pido.\n",
    "âœ… PySpark sÃ³ ganha quando o dataset Ã© grande demais para a RAM.\n",
    "\n",
    "ğŸš€ Resumo Final\n",
    "Se o Pandas foi mais rÃ¡pido, significa que:\n",
    "âœ… O dataset ainda nÃ£o Ã© grande o suficiente para ver a diferenÃ§a.\n",
    "âœ… O Spark local nÃ£o estÃ¡ a usar mÃºltiplos nÃ³s.\n",
    "âœ… O overhead inicial do Spark ainda nÃ£o compensa.\n",
    "\n",
    "ğŸ”´ O que fazer?\n",
    "â¡ï¸ Aumentar o dataset para 50 milhÃµes de linhas.\n",
    "â¡ï¸ Executar o PySpark num cluster real.\n",
    "â¡ï¸ Testar queries mais complexas, como joins pesados e machine learning.\n",
    "\n",
    "Se quiseres, posso ajudar a gerar um dataset maior para veres o verdadeiro poder do PySpark! ğŸš€ğŸ”¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
